{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\python\\python39\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: torch in c:\\python\\python39\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\python\\python39\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: pandas in c:\\python\\python39\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: nltk in c:\\python\\python39\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: filelock in c:\\python\\python39\\lib\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\python\\python39\\lib\\site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python\\python39\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sairaj\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python\\python39\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python\\python39\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\python\\python39\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\python\\python39\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\python\\python39\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\python\\python39\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sairaj\\appdata\\roaming\\python\\python39\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\python\\python39\\lib\\site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\python\\python39\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\python\\python39\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\python\\python39\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\python\\python39\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\python\\python39\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python\\python39\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sairaj\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python\\python39\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python\\python39\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: click in c:\\python\\python39\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sairaj\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sairaj\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python\\python39\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\python39\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\python39\\lib\\site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\python39\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\python39\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python\\python39\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch scikit-learn pandas nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Year  Population  Murder  Rape  Robbery  Assault  Burglary  CarTheft\n",
      "0  1965    18073000     836  2320    28182    27464    183443     58452\n",
      "1  1966    18258000     882  2439    30098    29142    196127     64368\n",
      "2  1967    18336000     996  2665    40202    31261    219157     83775\n",
      "3  1968    18113000    1185  2527    59857    34946    250918    104877\n",
      "4  1969    18321000    1324  2902    64754    36890    248477    115400\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the crime data from a CSV file\n",
    "# Assume the dataset contains columns: ['description', 'latitude', 'longitude', 'crime_type', 'date']\n",
    "crime_data = pd.read_csv('crime_data.csv')\n",
    "\n",
    "# Display the first few rows of the data\n",
    "print(crime_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'description' does not exist in crime_data. Available columns are: Index(['Year', 'Population', 'Murder', 'Rape', 'Robbery', 'Assault',\n",
      "       'Burglary', 'CarTheft'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Basic text cleaning: lowercase, remove special characters\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove non-alphabetic characters\n",
    "    return text\n",
    "\n",
    "# Check if 'description' column exists\n",
    "if 'description' in crime_data.columns:\n",
    "    # Clean the crime descriptions\n",
    "    crime_data['clean_description'] = crime_data['description'].apply(clean_text)\n",
    "\n",
    "    # Tokenization and removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    crime_data['tokens'] = crime_data['clean_description'].apply(\n",
    "        lambda x: [word for word in word_tokenize(x) if word not in stop_words])\n",
    "\n",
    "    # Display cleaned text data\n",
    "    print(crime_data[['description', 'clean_description']].head())\n",
    "else:\n",
    "    print(\"Column 'description' does not exist in crime_data. Available columns are:\", crime_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\python\\python39\\lib\\site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\python\\python39\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: torch in c:\\python\\python39\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in c:\\python\\python39\\lib\\site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\python\\python39\\lib\\site-packages (from transformers) (0.24.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python\\python39\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sairaj\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python\\python39\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python\\python39\\lib\\site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in c:\\python\\python39\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\python\\python39\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\python\\python39\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\python\\python39\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\sairaj\\appdata\\roaming\\python\\python39\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\python\\python39\\lib\\site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\python\\python39\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\python\\python39\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\python\\python39\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sairaj\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python\\python39\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python\\python39\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python\\python39\\lib\\site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python\\python39\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python\\python39\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python\\python39\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tenacity in c:\\python\\python39\\lib\\site-packages (9.0.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\sairaj\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "from huggingface_hub import login\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "\n",
    "# Correct model identifier (use a known working model)\n",
    "model_identifier = 'gpt2'\n",
    "\n",
    "# Your Hugging Face access token\n",
    "access_token = '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'\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=access_token, add_to_git_credential=True)\n",
    "\n",
    "@retry(stop=stop_after_attempt(5), wait=wait_fixed(2))\n",
    "def load_model_and_tokenizer(model_identifier, access_token):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_identifier, use_auth_token=access_token)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_identifier, use_auth_token=access_token)\n",
    "        \n",
    "        # Add padding token if not present\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        return tokenizer, model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model and tokenizer: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load the tokenizer and model with retry logic\n",
    "tokenizer, model = load_model_and_tokenizer(model_identifier, access_token)\n",
    "\n",
    "# Example crime_data DataFrame\n",
    "crime_data = pd.DataFrame({\n",
    "    'clean_description': [\"description1\", \"description2\"]\n",
    "})\n",
    "\n",
    "# Tokenize the cleaned crime descriptions\n",
    "inputs = tokenizer(crime_data['clean_description'].tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Generate embeddings for the crime descriptions\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.logits\n",
    "\n",
    "# Store the embeddings\n",
    "crime_data['embeddings'] = embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\sairaj\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from huggingface_hub import login\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "\n",
    "# Correct model identifier (use a known working model)\n",
    "model_identifier = 'gpt2'\n",
    "\n",
    "# Your Hugging Face access token\n",
    "access_token = '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'\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token=access_token, add_to_git_credential=True)\n",
    "\n",
    "@retry(stop=stop_after_attempt(5), wait=wait_fixed(2))\n",
    "def load_model_and_tokenizer(model_identifier, access_token):\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_identifier, use_auth_token=access_token)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_identifier, use_auth_token=access_token)\n",
    "        \n",
    "        # Add padding token if not present\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        return tokenizer, model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model and tokenizer: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load the tokenizer and model with retry logic\n",
    "tokenizer, model = load_model_and_tokenizer(model_identifier, access_token)\n",
    "\n",
    "# Example crime_data DataFrame\n",
    "crime_data = pd.DataFrame({\n",
    "    'clean_description': [\"description1\", \"description2\"],\n",
    "    'latitude': [34.0522, 36.1699],\n",
    "    'longitude': [-118.2437, -115.1398],\n",
    "    'crime_type': ['theft', 'assault']\n",
    "})\n",
    "\n",
    "# Tokenize the cleaned crime descriptions\n",
    "inputs = tokenizer(crime_data['clean_description'].tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Generate embeddings for the crime descriptions\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.logits\n",
    "\n",
    "# Convert embeddings to a 2D array\n",
    "embeddings = embeddings.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# Combine embeddings with geographical data (latitude, longitude)\n",
    "features = np.hstack([embeddings, crime_data[['latitude', 'longitude']].values])\n",
    "\n",
    "# The target variable could be future crime occurrences, crime types, etc.\n",
    "# For simplicity, we'll assume we're predicting the type of crime\n",
    "target = crime_data['crime_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a RandomForest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Prediction Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Crime Types: ['theft' 'theft']\n"
     ]
    }
   ],
   "source": [
    "# New sample data for prediction (new crime descriptions and locations)\n",
    "new_data = {\n",
    "    'description': [\"Suspicious activity near a shopping mall\", \"Attempted theft in residential area\"],\n",
    "    'latitude': [41.881832, 41.878113],\n",
    "    'longitude': [-87.623177, -87.629799]\n",
    "}\n",
    "\n",
    "# Preprocess and tokenize the new data\n",
    "new_data_df = pd.DataFrame(new_data)\n",
    "new_data_df['clean_description'] = new_data_df['description'].apply(clean_text)\n",
    "new_inputs = tokenizer(new_data_df['clean_description'].tolist(), return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "# Generate embeddings for the new data\n",
    "with torch.no_grad():\n",
    "    new_outputs = model(**new_inputs)\n",
    "    new_embeddings = new_outputs.logits\n",
    "\n",
    "# Convert new embeddings to a 2D array\n",
    "new_embeddings = new_embeddings.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# Combine new embeddings with geographical data\n",
    "new_features = np.hstack([new_embeddings, new_data_df[['latitude', 'longitude']].values])\n",
    "\n",
    "# Predict crime types or hotspot potential for new data\n",
    "new_predictions = clf.predict(new_features)\n",
    "print(\"Predicted Crime Types:\", new_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
